{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d625e60-afcb-4b22-8b8f-56a719327ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apurbakoirala/anaconda3/envs/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Model\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310be4d4-a927-403a-a963-e633e4e1f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "# path = kagglehub.dataset_download(\"cubeai/dog-nose-detection-for-yolov8\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd32c13-7409-4118-8496-16d5a1edcb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arcface DogNose.ipynb  Prediction Model.ipynb \u001b[34mcontent\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732ce60b-4cc9-48c6-b0cb-f5dd8cc833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 256\n",
    "NUM_CLASSES = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1fc588-797f-4eff-9589-81cb7747ce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2916 files belonging to 2 classes.\n",
      "Found 184 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 14:16:32.081088: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-03-13 14:16:32.081116: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-03-13 14:16:32.081119: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741855592.081484 9818873 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1741855592.081513 9818873 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_path):\n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        data_path,\n",
    "        label_mode='int',\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "train_ds = load_dataset('content/train')\n",
    "val_ds = load_dataset('content/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1d14d2-13fa-401a-91b2-30bdb84d3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomContrast(0.2),\n",
    "    layers.RandomZoom(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e68e67d-8ad7-4b04-ac92-a63df2bf055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "normalize = layers.Rescaling(1./127.5, offset=-1)  # MobileNetV3 scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4190d0-5095-4e67-baca-fe44908d6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    image = augmentation(image)\n",
    "    image = normalize(image)\n",
    "    label = tf.cast(label, tf.int32)  # Convert labels to int32\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1013b5d3-4adb-4b19-8b96-f05fb539ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(preprocess).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(preprocess).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46476d2-be51-456c-b758-bfe0256e46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(layers.Layer):\n",
    "    def __init__(self, n_classes, s=30.0, m=0.50, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name='arcface_weights'\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, labels=None):\n",
    "        # Normalize features and weights\n",
    "        x_norm = tf.nn.l2_normalize(inputs, axis=1)\n",
    "        w_norm = tf.nn.l2_normalize(self.w, axis=0)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        cos_theta = tf.matmul(x_norm, w_norm)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Convert labels to one-hot\n",
    "            one_hot = tf.one_hot(labels, depth=self.n_classes)\n",
    "            \n",
    "            # Calculate theta + margin\n",
    "            theta = tf.acos(tf.clip_by_value(cos_theta, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "            margin_theta = theta + self.m\n",
    "            cos_margin = tf.cos(margin_theta)\n",
    "            \n",
    "            # Apply margin to correct class\n",
    "            final_logits = self.s * (one_hot * cos_margin + (1 - one_hot) * cos_theta)\n",
    "            return final_logits\n",
    "        \n",
    "        return self.s * cos_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e0671f-2dd7-4b5e-9ded-e0617b3bc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # MobileNetV3 backbone\n",
    "    base_model = tf.keras.applications.MobileNetV3Small(\n",
    "        input_shape=(*IMG_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Freeze base layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Embedding layer\n",
    "    inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "    x = base_model(inputs)\n",
    "    embeddings = layers.Dense(EMBEDDING_SIZE)(x)\n",
    "    \n",
    "    # ArcFace classification head\n",
    "    arcface = ArcFace(n_classes=NUM_CLASSES, name='arc_face')(embeddings)\n",
    "    \n",
    "    return Model(inputs, arcface)  # Single output during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df267476-7a4a-4ccc-b24c-5489f5328f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobileNetV3Small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ arc_face (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ArcFace</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,600</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobileNetV3Small (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │       \u001b[38;5;34m939,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m147,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ arc_face (\u001b[38;5;33mArcFace\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m25,600\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,112,432</span> (4.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,112,432\u001b[0m (4.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">173,312</span> (677.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m173,312\u001b[0m (677.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> (3.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m939,120\u001b[0m (3.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a998012a-4742-4679-8a2b-c5bfbf65842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, n_classes, s=30.0, m=0.50, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ce = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Convert y_true to int32\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        \n",
    "        # Convert labels to one-hot\n",
    "        one_hot = tf.one_hot(y_true, depth=self.n_classes)\n",
    "        \n",
    "        # Calculate theta + margin\n",
    "        cos_theta = y_pred / self.s  # Reverse scaling\n",
    "        theta = tf.acos(tf.clip_by_value(cos_theta, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        margin_theta = theta + self.m\n",
    "        cos_margin = tf.cos(margin_theta)\n",
    "        \n",
    "        # Apply margin to the correct class\n",
    "        adjusted_logits = self.s * (one_hot * cos_margin + (1 - one_hot) * cos_theta)\n",
    "        \n",
    "        # Compute loss\n",
    "        return self.ce(y_true, adjusted_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "482040f3-bd80-4ebb-8be5-0a10dfd5d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass NUM_CLASSES (100) to ArcFaceLoss\n",
    "losses = {\n",
    "    'arc_face': ArcFaceLoss(n_classes=NUM_CLASSES),  # Match the output name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5872763d-d379-4749-ad73-90326af377da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your model compilation:\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=losses,\n",
    "    metrics={'arc_face': ['accuracy']}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9434fe8-295c-4133-b87e-628bf51964f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b64a7eff-77af-4a2c-8e1f-b5db59a77e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 14:16:33.055238: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 130ms/step - accuracy: 0.9422 - loss: 2.6092 - val_accuracy: 1.0000 - val_loss: 2.8118e-07 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 2.6720e-07 - val_accuracy: 1.0000 - val_loss: 2.7340e-07 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 8.2026e-07 - val_accuracy: 1.0000 - val_loss: 2.9867e-07 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 2.6151e-07 - val_accuracy: 1.0000 - val_loss: 2.7081e-07 - learning_rate: 5.0000e-04\n",
      "Epoch 5/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 2.6480e-07 - val_accuracy: 1.0000 - val_loss: 2.7081e-07 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840730d7-3962-4a83-8372-bf8d4de3d968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
